{
 "cells": [
  {
   "cell_type": "code",
   "id": "7d1148a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:49.253233Z",
     "start_time": "2025-09-15T13:26:49.241230Z"
    }
   },
   "source": [
    "# 处理文本数据"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b8350010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:49.345793Z",
     "start_time": "2025-09-15T13:26:49.327681Z"
    }
   },
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.7.1\n",
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8de36451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:49.519894Z",
     "start_time": "2025-09-15T13:26:49.501863Z"
    }
   },
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "facc2e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:49.599530Z",
     "start_time": "2025-09-15T13:26:49.588528Z"
    }
   },
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:100])  # 打印前1000个字符"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "1d3f7bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:49.693855Z",
     "start_time": "2025-09-15T13:26:49.686857Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'(\\s)',text)\n",
    "\n",
    "print(\"Split result:\", result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split result: ['Hello,', ' ', 'world.', ' ', 'Is', ' ', 'this--', ' ', 'a', ' ', 'test?']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "19f0fcb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:49.773959Z",
     "start_time": "2025-09-15T13:26:49.768853Z"
    }
   },
   "source": [
    "resultTmp = re.split(r'([,.]|\\s)', text)\n",
    "result = [x for x in resultTmp if x.strip()]\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this--', 'a', 'test?']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "5fef7d26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:49.884063Z",
     "start_time": "2025-09-15T13:26:49.866067Z"
    }
   },
   "source": [
    "resultTmp = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [x.strip() for x in resultTmp if x.strip()]\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "4d0e15a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:49.979385Z",
     "start_time": "2025-09-15T13:26:49.969647Z"
    }
   },
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [x.strip() for x in preprocessed if x.strip()]\n",
    "print(\"Preprocessed text:\", preprocessed[:30])  # 打印前10个元素"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "371c3b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.026388Z",
     "start_time": "2025-09-15T13:26:50.014363Z"
    }
   },
   "source": [
    "print(len(preprocessed))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ce540abb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.074364Z",
     "start_time": "2025-09-15T13:26:50.058364Z"
    }
   },
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(\"Total unique words:\", len(all_words))\n",
    "print(\"First 10 unique words:\", all_words[:20])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 1130\n",
      "First 10 unique words: ['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "9cde5cd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.122378Z",
     "start_time": "2025-09-15T13:26:50.103363Z"
    }
   },
   "source": [
    "vocabulary = {token: i for i, token in enumerate(all_words)}\n",
    "for i, token in enumerate(all_words[:10]):\n",
    "    print(f\"{i}: {token}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: '\n",
      "3: (\n",
      "4: )\n",
      "5: ,\n",
      "6: --\n",
      "7: .\n",
      "8: :\n",
      "9: ;\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "1d5ed79e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.183183Z",
     "start_time": "2025-09-15T13:26:50.170185Z"
    }
   },
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.str_to_int = vocabulary\n",
    "        self.int_to_str = {i: token for token, i in vocabulary.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [x.strip() for x in preprocessed if x.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed ]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join([self.int_to_str[t] for t in tokens])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1',  text)\n",
    "        return text"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "de93f4c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.198330Z",
     "start_time": "2025-09-15T13:26:50.191183Z"
    }
   },
   "source": [
    "tokenizer = SimpleTokenizerV1(vocabulary)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "encoded = tokenizer.encode(text) \n",
    "print(\"Encoded tokens:\", encoded)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded tokens: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "5a2d5c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.230335Z",
     "start_time": "2025-09-15T13:26:50.224318Z"
    }
   },
   "source": [
    "decode = tokenizer.decode(encoded)\n",
    "print(\"Decoded text:\", decode)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "56d8e9e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.294318Z",
     "start_time": "2025-09-15T13:26:50.283317Z"
    }
   },
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "8a62c5d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.372879Z",
     "start_time": "2025-09-15T13:26:50.357874Z"
    }
   },
   "source": [
    "# tokernizer = SimpleTokenizerV1(vocabulary)\n",
    "\n",
    "# text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "# encoded = tokenizer.encode(text)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "d0937fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.388879Z",
     "start_time": "2025-09-15T13:26:50.381884Z"
    }
   },
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<unk>\"])\n",
    "\n",
    "vocabulary = {token: i for i, token in enumerate(all_tokens)} "
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "ea6414de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.404885Z",
     "start_time": "2025-09-15T13:26:50.396885Z"
    }
   },
   "source": [
    "len(vocabulary.items())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "3c69bd3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.436880Z",
     "start_time": "2025-09-15T13:26:50.430882Z"
    }
   },
   "source": [
    "for i, item in enumerate(list(vocabulary.items())[-5:]):\n",
    "    print(f\"{i}: {item}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ('younger', 1127)\n",
      "1: ('your', 1128)\n",
      "2: ('yourself', 1129)\n",
      "3: ('<|endoftext|>', 1130)\n",
      "4: ('<unk>', 1131)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "cb05878f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.515056Z",
     "start_time": "2025-09-15T13:26:50.496806Z"
    }
   },
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.str_to_int = vocabulary\n",
    "        self.int_to_str = {i: token for token, i in vocabulary.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [x.strip() for x in preprocessed if x.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<unk>\"\n",
    "            for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int.get(s) for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str.get(t) for t in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1',  text)\n",
    "        return text"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "da7c5b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.531048Z",
     "start_time": "2025-09-15T13:26:50.524050Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0eade4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.546994Z",
     "start_time": "2025-09-15T13:26:50.538051Z"
    }
   },
   "source": [
    "tokenizer = SimpleTokenizerV2(vocabulary)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\"Text to encode:\", text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to encode: Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "4ee14b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.610976Z",
     "start_time": "2025-09-15T13:26:50.594975Z"
    }
   },
   "source": [
    "tokenizer.encode(text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "af3c980c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:50.690569Z",
     "start_time": "2025-09-15T13:26:50.675573Z"
    }
   },
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>, do you like tea? <|endoftext|> In the sunlit terraces of the <unk>.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "781e6022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:26:56.814944Z",
     "start_time": "2025-09-15T13:26:50.775947Z"
    }
   },
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "c29b8605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:27:10.418822Z",
     "start_time": "2025-09-15T13:27:10.400077Z"
    }
   },
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(\"Encoded integers:\", integers)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded integers: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "c1e8cd8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:27:11.816716Z",
     "start_time": "2025-09-15T13:27:11.809697Z"
    }
   },
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(\"Decoded strings:\", strings)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded strings: Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "6d46369b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:27:14.780655Z",
     "start_time": "2025-09-15T13:27:14.770279Z"
    }
   },
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Encoded text length:\", len(enc_text))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text length: 5145\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "40da28f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:35:00.775971Z",
     "start_time": "2025-09-15T13:35:00.765680Z"
    }
   },
   "source": [
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x:{x}\")\n",
    "print(f\"  y:{y}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:[290, 4920, 2241, 287]\n",
      "  y:[4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:37:38.662848Z",
     "start_time": "2025-09-15T13:37:38.655302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ],
   "id": "e34a22cc186c8e88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T13:41:44.366328Z",
     "start_time": "2025-09-15T13:41:44.335243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "id": "68db171149ab79ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
